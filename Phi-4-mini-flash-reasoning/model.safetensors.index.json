{
  "metadata": {
    "total_size": 7706608640
  },
  "weight_map": {
    "model.embed_tokens.weight": "model-00001-of-00002.safetensors",
    "model.final_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.final_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.0.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.0.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.0.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.0.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.1.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.1.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.1.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.1.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.10.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.10.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.10.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.11.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.11.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.11.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.11.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.12.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.12.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.12.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.13.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.13.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.13.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.13.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.14.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.14.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.14.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.15.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.15.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.15.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.15.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.16.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.16.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.16.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.17.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.17.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.17.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.17.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.18.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.18.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.18.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.18.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.18.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.18.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.18.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.18.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.19.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.19.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.19.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.19.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.2.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.2.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.2.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.2.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.20.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.20.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.20.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.20.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.20.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.20.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.20.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.20.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.21.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.21.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.21.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.21.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.22.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.22.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.22.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.22.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.22.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.22.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.22.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.22.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.23.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.23.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.23.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.23.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.24.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.24.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.24.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.24.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.24.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.24.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.24.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.24.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.25.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.25.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.25.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.25.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.26.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.26.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.26.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.26.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.26.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.26.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.26.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.26.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.27.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.27.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.27.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.27.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.28.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.28.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.28.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.28.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.28.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.28.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.28.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.28.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.29.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.29.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.29.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.29.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.3.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.3.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.3.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.3.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.3.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.3.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.3.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.3.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.30.attn.in_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.30.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.30.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.30.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.30.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.30.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.30.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.30.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.Wqkv.bias": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.Wqkv.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.inner_cross_attn.lambda_k1": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.inner_cross_attn.lambda_k2": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.inner_cross_attn.lambda_q1": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.inner_cross_attn.lambda_q2": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.inner_cross_attn.subln.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.out_proj.bias": "model-00002-of-00002.safetensors",
    "model.layers.31.attn.out_proj.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.input_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.31.input_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.mlp.fc1.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.mlp.fc2.weight": "model-00002-of-00002.safetensors",
    "model.layers.31.post_attention_layernorm.bias": "model-00002-of-00002.safetensors",
    "model.layers.31.post_attention_layernorm.weight": "model-00002-of-00002.safetensors",
    "model.layers.4.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.4.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.4.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.4.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.5.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.5.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.5.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.5.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.6.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.6.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.6.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.7.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.7.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.7.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.7.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.A_log": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.D": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.conv1d.bias": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.conv1d.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.dt_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.dt_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.in_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.attn.x_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.8.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.8.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.8.post_attention_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.Wqkv.bias": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.Wqkv.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.inner_cross_attn.lambda_k1": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.inner_cross_attn.lambda_k2": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.inner_cross_attn.lambda_q1": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.inner_cross_attn.lambda_q2": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.inner_cross_attn.subln.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.out_proj.bias": "model-00001-of-00002.safetensors",
    "model.layers.9.attn.out_proj.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.input_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.9.input_layernorm.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.mlp.fc1.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.mlp.fc2.weight": "model-00001-of-00002.safetensors",
    "model.layers.9.post_attention_layernorm.bias": "model-00001-of-00002.safetensors",
    "model.layers.9.post_attention_layernorm.weight": "model-00001-of-00002.safetensors"
  }
}
